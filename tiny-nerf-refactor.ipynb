{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"tiny_nerf.ipynb","provenance":[]},"widgets":{"application/vnd.jupyter.widget-state+json":{"0b499c0b045f47739b9f02e3044ab202":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2d10a9d7f90d46068657a276708e2830":{"model_module":"@jupyter-widgets/controls","model_name":"FloatSliderModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatSliderModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"FloatSliderView","continuous_update":true,"description":"radius","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_88f12e14df244cbd9eb1411f2eebcba6","max":5,"min":3,"orientation":"horizontal","readout":true,"readout_format":".2f","step":0.01,"style":"IPY_MODEL_d6695da9ffa041a1b5bd42d829b06f20","value":4}},"3e1c885192e14e779f404204ea75a2a0":{"model_module":"@jupyter-widgets/controls","model_name":"IntProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"IntProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"100%","description_tooltip":null,"layout":"IPY_MODEL_4469cf78c9a547cf913738b7460c9655","max":120,"min":0,"orientation":"horizontal","style":"IPY_MODEL_eb7f40f7889c4912bedf30d4b310e189","value":120}},"3e97596c7ad7412d944dee129187bf78":{"model_module":"@jupyter-widgets/controls","model_name":"SliderStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"SliderStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":"","handle_color":null}},"4469cf78c9a547cf913738b7460c9655":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"541f62441bf7410eb60fa1dc0ac6806e":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","state":{"_dom_classes":["widget-interact"],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_c9945007a138443d9fbb8bb31af74cc3","IPY_MODEL_76b0b54cd6e7403aa097e3ac523698f2","IPY_MODEL_2d10a9d7f90d46068657a276708e2830","IPY_MODEL_c6097730a1244ceeba6cc840ae3f3a12"],"layout":"IPY_MODEL_0b499c0b045f47739b9f02e3044ab202"}},"76b0b54cd6e7403aa097e3ac523698f2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatSliderModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatSliderModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"FloatSliderView","continuous_update":true,"description":"phi","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_a45b06e19d5d4098b8d162d0d9e73d66","max":0,"min":-90,"orientation":"horizontal","readout":true,"readout_format":".2f","step":0.01,"style":"IPY_MODEL_3e97596c7ad7412d944dee129187bf78","value":-30}},"7d2b3c3af3e045caae9814cb38080e46":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7dc4653f29174908bc749765b7865416":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"88f12e14df244cbd9eb1411f2eebcba6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9031916d729c478a9b3866f4d91d8fe3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3e1c885192e14e779f404204ea75a2a0","IPY_MODEL_a59798fdf1b841ec8911d74e0df8753d"],"layout":"IPY_MODEL_f326611178244edc9ab69d8f7f7c79d9"}},"a45b06e19d5d4098b8d162d0d9e73d66":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a59798fdf1b841ec8911d74e0df8753d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7dc4653f29174908bc749765b7865416","placeholder":"â€‹","style":"IPY_MODEL_7d2b3c3af3e045caae9814cb38080e46","value":" 120/120 [00:35&lt;00:00,  3.42it/s]"}},"babb6b978ae649a88eaea62c2c295b2f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c6097730a1244ceeba6cc840ae3f3a12":{"model_module":"@jupyter-widgets/output","model_name":"OutputModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/output","_model_module_version":"1.0.0","_model_name":"OutputModel","_view_count":null,"_view_module":"@jupyter-widgets/output","_view_module_version":"1.0.0","_view_name":"OutputView","layout":"IPY_MODEL_1b002a5c05f248e781ee20127dbb43d7","msg_id":"","outputs":[{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAWgAAAFnCAYAAACLs9MAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","metadata":{"tags":[]},"output_type":"display_data","text/plain":"<Figure size 1440x432 with 1 Axes>"}]}},"c9945007a138443d9fbb8bb31af74cc3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatSliderModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatSliderModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"FloatSliderView","continuous_update":true,"description":"theta","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_babb6b978ae649a88eaea62c2c295b2f","max":360,"min":0,"orientation":"horizontal","readout":true,"readout_format":".2f","step":0.01,"style":"IPY_MODEL_ca487a41d9894ba8a5e28a05fd7ed43e","value":157.57}},"ca487a41d9894ba8a5e28a05fd7ed43e":{"model_module":"@jupyter-widgets/controls","model_name":"SliderStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"SliderStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":"","handle_color":null}},"d6695da9ffa041a1b5bd42d829b06f20":{"model_module":"@jupyter-widgets/controls","model_name":"SliderStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"SliderStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":"","handle_color":null}},"eb7f40f7889c4912bedf30d4b310e189":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"f326611178244edc9ab69d8f7f7c79d9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9635016,"sourceType":"datasetVersion","datasetId":5882440},{"sourceId":9635128,"sourceType":"datasetVersion","datasetId":5882763},{"sourceId":9816761,"sourceType":"datasetVersion","datasetId":5988240},{"sourceId":10202622,"sourceType":"datasetVersion","datasetId":6304939},{"sourceId":10220169,"sourceType":"datasetVersion","datasetId":6317904},{"sourceId":12010789,"sourceType":"datasetVersion","datasetId":6318300}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"##Tiny NeRF\nThis is a simplied version of the method presented in *NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis*\n\n[Project Website](http://www.matthewtancik.com/nerf)\n\n[arXiv Paper](https://arxiv.org/abs/2003.08934)\n\n[Full Code](github.com/bmild/nerf)\n\nComponents not included in the notebook\n*   5D input including view directions\n*   Hierarchical Sampling\n\n\n","metadata":{"id":"lLDTVWKq7-ei","editable":false}},{"cell_type":"markdown","source":"________________________________________________________________________________\n# Cambios/comentarios que dejamos\n\n## Cambios grandes\n\n- Cambiamos el config_loader para que pueda crearse fÃ¡cil una configuraciÃ³n como un objeto con parÃ¡metros. La clase ahora se llama ConfigurationLoader.\n\n- Cambiamos el tracker de mÃ©tricas para que guarde mÃ¡s cosas. Fijate que el loss, PSNR ahora se guarda ahÃ­.\n\n- En el ciclo de entrenamiento, dividimos la parte de train y la parte de test en dos funciones separadas.\n\n- La funciÃ³n de renderizado de imÃ¡genes ahora solo hace el plot. Al plot agregamos tambiÃ©n una grÃ¡fico del loss.\n\n- La informaciÃ³n sobre PSNR y loss ahora se guardan en todas iteraciones (usando el monitor) pero se muestran como antes (cada 50 iters). Fijate que las curvas son mÃ¡s suaves.\n\n\n## Comentarios menores y dudas\n\n- Dejamos cosas comentadas. La idea es que veas los cambios. Cuando estÃ©s conforme, borrÃ¡ las porciones de cÃ³digo comentadas.\n\n- El valor de 1024*8 de chunk_size de donde sale? Tiene que ver con el tamaÃ±o de las imÃ¡genes? HabrÃ­a que parametrizarlo? HabrÃ­a que ponerlo en el configurationLoader?\n\n    -) Y por quÃ© multiplica por 8?\n\n- Importante: necesitamos entender quÃ© es lo que retorna la funciÃ³n volume_render (y por tanto render_ray).\n\n- Revisar bien quÃ© cosas deberÃ­an estar como variables globales, y quÃ© cosas deberÃ­an pasarse como parÃ¡metros.\n\n    - lista de imÃ¡genes? trainingMonitor? configurationLoader?\n\n","metadata":{"editable":false}},{"cell_type":"code","source":"import os, sys\nimport json\nimport torch\nimport cv2\nimport pandas as pd\nimport json\nimport imageio\nimport time\nimport random\n\nfrom tqdm.notebook import tqdm\nimport numpy as np\nimport matplotlib.pyplot as plt","metadata":{"id":"bZNXlxmEj0FC","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_random_seed():\n    return random.randint(1, 1000)\n\ndef set_random_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nif device == 'cuda':\n    torch.cuda.empty_cache()\nprint(device)","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Loading dataset","metadata":{"editable":false}},{"cell_type":"markdown","source":"## Load default data","metadata":{"editable":false}},{"cell_type":"code","source":"if not os.path.exists('tiny_nerf_data.npz'):\n    !wget http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/ECCV20/nerf/tiny_nerf_data.npz\nexp_name = 'tiny_nerf_data'","metadata":{"id":"5mTxAwgrj4yn","outputId":"fbb14c52-6ca3-46c2-cf50-9cc3955fdc02","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load custom data (in progress)","metadata":{"editable":false}},{"cell_type":"markdown","source":"Introduce your experiment name (name of the dataset where images and poses are) and the desire factor to downsize the images in the dataset, 1 keeps the size of the original image and the more this number increase the lower the final resolution of the images would be.\n\n*(Keep in mind that more resolution also means more GPU usage so adjust chunk_size in configuration and your Accelerator accordingly)\n\n**(The requiered poses are generated by the script given in https://github.com/NVlabs/instant-ngp/blob/master/docs/nerf_dataset_tips.md)\n\nTODO : find a way to adjust focal and load dataset from link","metadata":{"editable":false}},{"cell_type":"code","source":"# exp_name is the name of the dataset\nexp_name = 'mate'\ndataset_name = 'mate-dataset'\n\n# Default value no resize\nresize_ratio = 16","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load custom data (poses generated by script and stored in transforms.json)\nfocal = 130 # Hardcoded (TODO)\n\n# Path to your .json file\njson_file_path = \"/kaggle/input/\"+dataset_name+\"/transforms.json\"\n\n# Open and load the JSON data\nwith open(json_file_path, 'r') as file:\n    data = json.load(file)\n    raw_poses = data['frames']\n\ndef get_pose_from_image(image_name):\n    found = False\n    i = 0\n    number_poses = len(raw_poses)\n    raw_pose = False\n    while not found and i < number_poses:\n        img_pose_path = raw_poses[i]['file_path']\n        if img_pose_path.endswith(image_name):\n            raw_pose = raw_poses[i]['transform_matrix']\n            found = True\n        else:\n            i = i + 1\n    return raw_pose\n\nexp_image_dir = \"/kaggle/input/\"+dataset_name+\"/images/\"\nexp_image_files = os.listdir(exp_image_dir)\n\nimages_list = []\nposes_list = []\nindex = 0\n\nfor image_file in exp_image_files: \n    image_path = os.path.join(exp_image_dir, image_file)\n    \n    # Search pose for image\n    raw_pose = get_pose_from_image(image_file)\n    \n    # Load the image \n    img = cv2.imread(image_path)\n    (height, width, _) = img.shape\n    resized_height = height / resize_ratio\n    resized_width = width / resize_ratio\n    resized_dimensions = (int(resized_width),int(resized_height))\n    \n    # Prune if image exist but no pose is asocciated\n    if img is not None and raw_pose:\n        # Convert BGR to RGB since OpenCV loads images in BGR format\n        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n        # Resize image if needed (also the original code assumes the images are in float32)\n        img_rgb = cv2.resize(img_rgb, resized_dimensions, interpolation = cv2.INTER_CUBIC).astype(np.float32)\n        img_rgb /= 255\n\n        # Append the image to the list\n        images_list.append(img_rgb)\n        poses_list.append(raw_pose)\n\n# Convert the list of images to a NumPy array with shape (num_images, width, height, rgb)\nimages_array = np.array(images_list)\nposes_array = np.array(poses_list).astype(np.float32)\n\nprint(images_array.dtype)\nprint(poses_array.dtype)\n\n# Check the shape of the array to confirm\nprint(images_array.shape, poses_array.shape)\n\noutput_path = '/kaggle/working/'+exp_name+'.npz'\n\nnp.savez_compressed(output_path, images=images_array, poses=poses_array, focal=focal)","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load Input Images and Poses","metadata":{"id":"t2dgdCDi-m3T","editable":false}},{"cell_type":"code","source":"# Loading dataset (either using custom or given data)\nload_path = exp_name + '.npz'\ndata = np.load(load_path)\n\n# Retriving images, poses, focal length and dimension for training\nimages = data['images']\n\nprint(\"1.\", len(images))\n\nposes = data['poses']\nposes = torch.from_numpy(poses).to(device)\n\nfocal = data['focal']\nfocal = torch.from_numpy(focal).to(device)\nH, W = images.shape[1:3]\n\nprint(images.shape, poses.shape, focal)\n\namount_images = images.shape[0]-1 #The last one is for holdout view\n\nprint(amount_images)\n\n# Getting pose and image to compare in holdout view \ntestimg = images[amount_images-4] # por quÃ© es -4?\ntestimg = torch.from_numpy(testimg).to(device)\n\ntestpose =  poses[amount_images-4]\n\n# Trim the amount of images to not contain the holdout image\nimages = torch.from_numpy(images[:amount_images]).to(device)\nposes = poses[:amount_images]\n\nprint(\"2.\", len(images))\n\n\n# Show image used for holdout view\nplt.imshow(testimg.detach().cpu().numpy())\nplt.show()","metadata":{"id":"jj1lof2ej0FI","outputId":"6eef311a-11bd-495f-97e8-6b23029b3936","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Encoding","metadata":{"editable":false}},{"cell_type":"code","source":"# Comentar para quÃ© sirve esta funciÃ³n\n# Cambiar el nombre a positionalEncoding\n# Comentar quÃ© es el parÃ¡metro x y de quÃ© tipo/estructura se trata\n# CambiÃ© el for interno que no servÃ­a para nada.\n# CambiÃ© el nombre de la variable de salida.\n\n# Function that applies the positional encoding talked in the paper (5D coordiantes to a higher dimension)\n# x is the 5D coordinates\ndef positionalEncoding(x):\n#     rets = [x]\n    output = [x]\n    # We follow (sin(2^L-1)*Ï€*x) for each coordinate in x\n    for i in range(config_loader.L_embed):\n    #    for fn in [tf.sin, tf.cos]:\n    #        rets.append(fn(2.**i * x))\n            output.append(torch.sin(2.**i * x))\n            output.append(torch.cos(2.**i * x))     \n   # return tf.concat(rets, -1)\n    output = torch.cat(output, -1)\n    return output","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Configuration","metadata":{"editable":false}},{"cell_type":"markdown","source":"## Definitions","metadata":{"editable":false}},{"cell_type":"code","source":"# Class for setting up configuration of experiments\n# TODO see which arguments store or change (and how to do it)\nclass ConfigurationLoader:\n    \n    def __init__(self,\n                 seed,\n                 L_embed = 6, learning_rate = 1e-3, \n                 N_samples = 32, N_iters = 1000, \n                 iterations_to_plot_image = 100, \n                 chunk_size = 512, \n                 image_selection_mode = 'random'):\n\n        self.seed = seed\n        self.L_embed = L_embed\n        self.learning_rate = learning_rate\n        self.N_samples = N_samples\n\n        # Max iteration\n        self.N_iters = N_iters\n        \n        # Plot every i iterations\n        self.iterations_to_plot_image = iterations_to_plot_image\n        \n        # Amount of rays for each ray (decrease if running out of memory)\n        self.chunk_size = chunk_size\n\n        # image selection mode\n        self.image_selection_mode = image_selection_mode\n\n        # embedding function\n        self.embed_fn  = positionalEncoding\n        \n    def get_configuration(self) :\n        configuration_string = {\n            \"seed\" : self.seed,\n            \"L_embed\" : self.L_embed,\n            \"learning_rate\" : self.learning_rate,\n            \"N_samples\" : self.N_samples,\n            \"chunk_size\" : self.chunk_size,\n            \"image_selection_mode\" : self.image_selection_mode\n        }\n        return configuration_string\n   \n    def load_configuration(self, path) :\n        if os.path.exists(path):\n            with open(path, 'r') as file:\n                data = json.load(file)\n                self.seed = data['seed']\n                self.L_embed = data['L_embed']\n                self.learning_rate = data['learning_rate']\n                self.N_samples = data['N_samples']\n                self.chunk_size = data['chunk_size']\n                self.image_selection_mode = data['image_selection_mode']\n            print('Configuration loaded from ',path)\n            return True\n        else:\n            print('Configuration file not found')\n            return False\n        ","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Default configuration","metadata":{"editable":false}},{"cell_type":"markdown","source":"Setup custom configuration or load configuration from a file (if configuration file not found we setup a default configuration)","metadata":{"editable":false}},{"cell_type":"code","source":"path_to_config = input(\"Path to .json file containing configuration\")\n\nrandom_seed = generate_random_seed()\nset_random_seed(random_seed)\nconfig_loader = ConfigurationLoader(random_seed) # default configuration \n\n\nif (config_loader.load_configuration(path_to_config)):\n    set_random_seed(config_loader.seed)\nprint(config_loader.seed)\n","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Auxilar functions","metadata":{"editable":false}},{"cell_type":"markdown","source":"Functions for tracking data","metadata":{"editable":false}},{"cell_type":"code","source":"def get_current_time():\n    format_time = \"%Y-%m-%d-%H-%M-%S\"\n    return time.strftime(format_time, time.localtime())\n    \n# Class for storing information of experiments\n# class DebugLogger:\n\nclass TrainingMonitor:    \n    def __init__(self, exp_name):\n        self.exp_name = exp_name\n        self.exp_start_time = get_current_time()\n        \n        snapshot_columns = ['Time','Iteration','Loss','PSNR']\n        \n        # Create an empty dataframe with data to track\n        self.exp_data = pd.DataFrame(columns=snapshot_columns)\n\n        self.psnrs = []\n        self.iternums = []\n        self.losses = []\n\n    def get_psnrs(self):\n        return self.psnrs\n\n    def get_losses(self):\n        return self.losses\n\n    def get_iternums(self):\n        return self.iternums\n\n    def save_psnr(self, psnr):\n        self.psnrs.append(psnr)\n\n    def save_loss(self, loss):\n        self.losses.append(loss)\n\n    def save_iternum(self, iternum):\n        self.iternums.append(iternum)\n        \n    def store_snapshot(self, iter_num, loss, psnr):\n        snapshot = {'Time':get_current_time(),'Iteration':iter_num,'Loss':loss,'PSNR':psnr}\n        self.exp_data = pd.concat([self.exp_data, pd.DataFrame([snapshot])], ignore_index=True)\n        self.exp_data.to_csv(self.exp_name+\"(\"+str(self.exp_start_time)+\")\"+\".csv\",index=False)\n        \n    #Store the model configuration also\n    def save_exp_configuration(self, config_loader):\n        with open(self.exp_name+\"(\"+str(self.exp_start_time)+\")\"+\"_configuration.json\", mode=\"w\", encoding=\"utf-8\") as config_json_file:\n            json.dump(config_loader.get_configuration(), config_json_file)\n","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# NerfModel","metadata":{"editable":false}},{"cell_type":"code","source":"class TinyNerfModel(torch.nn.Module):\n    def __init__(self, D=8, W=256, L_embed=6):\n        \n        super(TinyNerfModel, self).__init__()\n        input_dim = 3 + 3* 2 * L_embed\n\n        # Track current dimension as we go.\n        current_dim = input_dim\n        self.layers = torch.nn.ModuleList()\n\n        # Creating hidden layers and skip layers\n        # current_dim is tracking the dimension of each layer to allow connection between skip layer and hidden layer\n        for i in range(D):\n            \n            self.layers.append(torch.nn.Linear(current_dim, W))\n\n            # Skip layer \n            if (i % 4 == 0) and (i > 0):\n                current_dim = W + input_dim\n            else:\n                current_dim = W\n        \n        # Final layer with linear activation (RGBÎ±)\n        self.final_layer = torch.nn.Linear(current_dim, 4)\n        \n        self.D = D\n        self.W = W\n        self.input_dim = input_dim\n\n    def forward(self, x):\n        inputs = x\n        outputs = x\n\n        # We pass forward each layer\n        for i, layer in enumerate(self.layers):\n            outputs = layer(outputs)\n            outputs = torch.nn.functional.relu(outputs)\n\n            # Skip layers concats \n            if (i % 4 == 0) and (i > 0):\n                outputs = torch.cat([outputs, inputs], dim=-1)\n        \n        outputs = self.final_layer(outputs)\n        return outputs","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Auxiliar function to model","metadata":{"editable":false}},{"cell_type":"code","source":"from torchmetrics.image import StructuralSimilarityIndexMeasure\n\ndef cumprod_exclusive(tensor: torch.Tensor) -> torch.Tensor:\n    r\"\"\"Mimick functionality of tf.math.cumprod(..., exclusive=True), as it isn't available in PyTorch.\n    Args:\n        tensor (torch.Tensor): Tensor whose cumprod (cumulative product, see `torch.cumprod`) along dim=-1 is to be computed.\n  \n    Returns:\n        cumprod (torch.Tensor): cumprod of Tensor along dim=-1, mimiciking the functionality of\n      tf.math.cumprod(..., exclusive=True) (see `tf.math.cumprod` for details).\n    \"\"\"\n    # TESTED\n    # Only works for the last dimension (dim=-1)\n    dim = -1\n    # Compute regular cumprod first (this is equivalent to `tf.math.cumprod(..., exclusive=False)`).\n    cumprod = torch.cumprod(tensor, dim)\n    # \"Roll\" the elements along dimension 'dim' by 1 element.\n    cumprod = torch.roll(cumprod, 1, dim)\n    # Replace the first element by \"1\" as this is what tf.cumprod(..., exclusive=True) does.\n    cumprod[..., 0] = 1.\n  \n    return cumprod\n\ndef get_image_random_index(images):\n    img_index = np.random.randint(images.shape[0])\n    return img_index\n\ndef get_image_by_index(img_i):\n    return images[img_i]\n\ndef get_pose_by_index(img_i):\n    return poses[img_i]\n\ndef has_to_plot(i, iterations_to_plot_image):\n    return (i % iterations_to_plot_image) == 0\n\ndef calculate_loss(img_1, img_2):\n    # alpha = 0.98\n    # beta = 1 - alpha\n    mse = torch.nn.functional.mse_loss(img_1, img_2)\n    # img_1 = img_1.unsqueeze(0)\n    # img_2 = img_2.unsqueeze(0)\n    # img_1 = torch.nn.functional.pad(img_1, (4, 4, 4, 4), mode='constant', value=0)\n    # img_2 = torch.nn.functional.pad(img_2, (4, 4, 4, 4), mode='constant', value=0)\n    # ssim_value = ssim(img_1,img_2)\n    # loss = mse * alpha + ssim_value * beta\n    loss = mse\n    return loss\n\n# Create the SSIM metric\nssim = StructuralSimilarityIndexMeasure(data_range=1.0).to(device)\n\n# def render_image_training(H, W, focal, pose, testimage, model):\n#     # We obtain the rays to query the model\n#     rays_o, rays_d = get_rays(H, W, focal, pose)\n#     rgb, depth, acc = render_rays(model, rays_o, rays_d, near=2., far=6., N_samples=config_loader.N_samples, chunk_size=config_loader.chunk_size)\n\n#     # Calculate loss between expected image and obtained\n#     loss = torch.nn.functional.mse_loss(rgb, testimage)\n\n#     # Peak signal-to-noise-ratio metric for comparison\n#     psnr = -10. * torch.log10(loss)\n\n#     # Store data\n#     training_monitor.store_snapshot(i, loss.item(), psnr.item())\n    \n#     psnrs.append(psnr.item())\n#     iternums.append(i)\n  \n#     plt.figure(figsize=(10,4))\n#     plt.subplot(121)\n#     plt.imshow(rgb.detach().cpu().numpy())\n#     plt.title(f'Iteration: {i}')\n#     plt.subplot(122)\n#     plt.plot(iternums, psnrs)\n#     plt.title('PSNR')\n#     plt.show()\n\n# def predict_testing_image_and_save_info(H, W, focal, pose, testimage, model, training_monitor):\n#     # We obtain the rays to query the model\n#     rays_o, rays_d = get_rays(H, W, focal, pose)\n#     rgb, depth, acc = render_rays(model, rays_o, rays_d, near=2., far=6., N_samples=config_loader.N_samples, chunk_size=config_loader.chunk_size)\n\n#     # Calculate loss between expected image and obtained\n#     loss = torch.nn.functional.mse_loss(rgb, testimage)\n\n#     # Peak signal-to-noise-ratio metric for comparison\n#     psnr = -10. * torch.log10(loss)\n\n#     # Store data\n#     training_monitor.store_snapshot(i, loss.item(), psnr.item())\n\ndef generate_checkpoint(model, iteration_number, loss):\n    path = '/kaggle/working/'+exp_name+'_checkpoint.pt'\n    #torch.save(model.state_dict(),path)\n    torch.save({\n            'epoch': iteration_number,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'loss': loss\n            }, path)\n\ndef load_checkpoint(model, path):\n    iteration_number = 0\n    if os.path.exists(path):\n        checkpoint = torch.load(path, weights_only=True)\n        model.load_state_dict(checkpoint['model_state_dict'])\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        iteration_number = checkpoint['epoch']\n        loss = checkpoint['loss']\n    \n        #model.load_state_dict(torch.load(path, weights_only=True))\n        #model.eval()\n        model.train()\n        print('Checkpoint found at '+str(iteration_number)+\" iterations\")\n    else:\n        print('No checkpoint to load, starting fresh')\n\n    return iteration_number\n\ndef calculate_and_save_training_metrics(rgb, test_image, iteration_number):\n    # Calculate loss between expected image and obtained\n    # loss = torch.nn.functional.mse_loss(rgb, test_image)\n    \n    # rgb = rgb.unsqueeze(0)\n    # test_image = testimg.unsqueeze(0)\n    # rgb = torch.nn.functional.pad(rgb, (4, 4, 4, 4), mode='constant', value=0)\n    # test_image = torch.nn.functional.pad(test_image, (4, 4, 4, 4), mode='constant', value=0)\n    # loss = ssim(rgb,test_image)\n\n    loss = calculate_loss(rgb,test_image)\n    \n    # Peak signal-to-noise-ratio metric for comparison\n    psnr = -10. * torch.log10(loss)\n    \n    # Store data\n    training_monitor.store_snapshot(iteration_number, loss.item(), psnr.item())\n    training_monitor.save_loss(loss.item())\n    training_monitor.save_psnr(psnr.item())\n    training_monitor.save_iternum(iteration_number)\n\ndef plot_image_and_metrics(rgb, iteration_number):\n\n    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(14, 4))\n\n    # Primer grÃ¡fico: Imagen predicha\n    ax1.imshow(rgb.detach().cpu().numpy())\n    ax1.set_title(f'Predicted Image (Iteration: {iteration_number})')\n    \n    # Segundo grÃ¡fico: PSNR\n    ax2.plot(training_monitor.get_iternums(), training_monitor.get_psnrs(), label='PSNR', color='blue')\n    ax2.set_title('PSNR')\n    ax2.set_xlabel('Iterations')\n    ax2.set_ylabel('PSNR')\n    ax2.legend()\n    \n    # Tercer grÃ¡fico: Loss\n    ax3.plot(training_monitor.get_iternums(), training_monitor.get_losses(), label='Loss', color='blue')\n    ax3.set_title('Loss')\n    ax3.set_xlabel('Iterations')\n    ax3.set_ylabel('Loss')\n    ax3.legend()\n    \n    # Ajustar diseÃ±o y mostrar\n    plt.tight_layout()\n    plt.show()\n\n\ndef render_image(H, W, focal, pose, test_image, model, \n                 iteration_number):\n    rays_o, rays_d = get_rays(H, W, focal, pose)\n    rgb, depth, acc = render_rays(model, rays_o, rays_d, near=2., far=6., N_samples=config_loader.N_samples, chunk_size=config_loader.chunk_size)\n\n    # if in_training:\n    #     calculate_and_save_training_metrics(rgb, test_image, iteration_number)\n\n    plot_image_and_metrics(rgb, iteration_number)\n\n    # Estos dos listas las pasamos al TrainingMonitor\n    # psnrs.append(psnr.item())\n    # iternums.append(i)\n\n    # fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n\n    # # First subplot\n    # ax1.imshow(rgb.detach().cpu().numpy())\n    # ax1.set_title(f'Predicted Image (Iteration: {iteration_number})')\n    \n    # # Second subplot\n    # ax2.plot(training_monitor.get_iternums(), training_monitor.get_psnrs(), label='PSNR')\n    # # ax2.plot(training_monitor.get_iternums(), training_monitor.get_losses(), label='Loss')\n    # ax2.set_title('PSNR and Loss')\n    \n    # # Adjust layout and display the figure\n    # plt.tight_layout()\n    # plt.show()\n  \n    # plt.figure(figsize=(10,4))\n    # plt.subplot(121)\n    # plt.imshow(rgb.detach().cpu().numpy())\n    # plt.title(f'Predicted Image (Iteration: {iteration_number})')\n    # plt.subplot(122)\n    # plt.plot(training_monitor.get_iternums(), training_monitor.get_psnrs())\n    # plt.plot(training_monitor.get_iternums(), training_monitor.get_losses())\n    # plt.title('PSNR')\n    # plt.show()\n\n# def render_image(H, W, focal, pose, model):\n#     # We obtain the rays to query the model\n#     rays_o, rays_d = get_rays(H, W, focal, pose)\n#     rgb, depth, acc = render_rays(model, rays_o, rays_d, near=2., far=6., N_samples=config_loader.N_samples, chunk_size=config_loader.chunk_size)\n\n#     # We print the rgb map\n#     plt.figure(figsize=(10,4))\n#     plt.subplot(121)\n#     plt.imshow(rgb)\n#     plt.show()\n\ndef get_rays(H, W, focal, c2w):\n    \n    # Create tensors that represent the image for accessing pixels\n    i, j = torch.meshgrid(torch.arange(W).to(c2w), torch.arange(H).to(c2w))\n    i, j =  i.transpose(-1, -2), j.transpose(-1, -2)\n    \n    # Normalize the coordinates between [-0.5,0.5] and scale it by the focal\n    x_cord = (i-W*.5)/focal\n    y_cord = -(j-H*.5)/focal #We invert the axis\n    z_cord = -torch.ones_like(i) #Points infront of camera\n    \n    # Generate director vectors in camera coordinate for each pixel\n    dirs = torch.stack([x_cord, y_cord, z_cord], -1)\n    \n    # We need to add a new dimension to the tensor to be compatible with transform matrix\n    dirs = dirs[..., np.newaxis, :]\n    \n    # Transform matrix to world coordinates (3x3) given by the pose\n    tf_matrix = c2w[:3,:3]\n    \n    # We translate the director vector of the rays from the camera coordinates to world coordinates\n    rays_d = torch.sum(dirs * tf_matrix, -1)\n    \n    # We set the origin of the rays\n    \n    cam_origin = c2w[:3,-1]\n    rays_o = cam_origin.expand(rays_d.shape)\n    return rays_o, rays_d\n\ndef volume_render(raw, ray_o, depth_values):\n    \n    # Compute opacities and colors\n    sigma_a = torch.nn.functional.relu(raw[...,3])\n    rgb = torch.sigmoid(raw[...,:3]) \n     \n    one_e_10 = torch.tensor([1e10], dtype=ray_o.dtype, device=ray_o.device)\n    dists = torch.cat((depth_values[..., 1:] - depth_values[..., :-1],\n                  one_e_10.expand(depth_values[..., :1].shape)), dim=-1)\n    alpha = 1. - torch.exp(-sigma_a * dists)\n    weights = alpha * cumprod_exclusive(1. - alpha + 1e-10)\n    \n    rgb_map = (weights[..., None] * rgb).sum(dim=-2)\n    depth_map = (weights * depth_values).sum(dim=-1)\n    acc_map = weights.sum(-1)\n    \n    return rgb_map, depth_map, acc_map\n\ndef render_rays(network_fn, rays_o, rays_d, near, far, N_samples, chunk_size, rand=False):\n\n    def get_minibatches(inputs: torch.Tensor, chunk_size = 1024 * 8):\n        return [inputs[i:i + chunk_size] for i in range(0, inputs.shape[0], chunk_size)]\n    \n    # Compute 3D query points (TODO maybe refactor to another function)\n    z_vals = torch.linspace(near, far, N_samples).to(rays_o)\n    if rand:\n        noise_shape = list(rays_o.shape[:-1]) + [N_samples]\n        # depth_values: (num_samples)\n        z_vals = z_vals \\\n        + torch.rand(noise_shape).to(rays_o) * (far\n            - near) / N_samples\n    \n    pts = rays_o[...,None,:] + rays_d[...,None,:] * z_vals[...,:,None]\n\n    # Run network\n    pts_flat = pts.reshape((-1,3))\n    pts_flat = config_loader.embed_fn(pts_flat)\n\n    # Split the encoded points into \"chunks\", run the model on all chunks, and\n    # concatenate the results (to avoid out-of-memory issues).\n    batches = get_minibatches(pts_flat, chunk_size=chunk_size)\n    predictions = []\n    for batch in batches:\n        predictions.append(model(batch))\n    raw = torch.cat(predictions, dim=0)\n\n    raw_shape = list(pts.shape[:-1]) + [4]\n    raw = torch.reshape(raw, raw_shape)\n    \n    # Do volume rendering based on opacities and color of sampled points\n    rgb_map, depth_map, acc_map = volume_render(raw, rays_o, z_vals)\n\n    return rgb_map, depth_map, acc_map","metadata":{"id":"R1avtwVoAQTu","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Here we optimize the model. We plot a rendered holdout view and its PSNR every specified iterations.","metadata":{"id":"3TSAyVcKAiyI","editable":false}},{"cell_type":"code","source":"def training_stage(H, W, focal, model):\n\n    # esto lo pusimos asÃ­ por las dudas, para el futuro.\n    if config_loader.image_selection_mode == 'random':\n        image_index = get_image_random_index(images)\n    elif config_loader.image_selection_mode == 'angle':\n        image_index =  get_image_random_index(images) # <<placeholder, estÃ¡ igual que el then.\n\n    target_image = get_image_by_index(image_index)\n    target_image_pose = get_pose_by_index(image_index)\n    target_image.to(device)\n    target_image_pose.to(device)\n    rays_o, rays_d = get_rays(H, W, focal, target_image_pose)\n    \n    rgb, depth, acc = render_rays(model, rays_o, rays_d, near=2., far=6., N_samples=config_loader.N_samples, chunk_size=config_loader.chunk_size, rand=True)\n    \n    loss = calculate_loss(rgb,target_image)\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n\ndef testing_stage(H, W, focal, model, testimg, testpose, iteration_number, start_time):\n    # testing with holdout image\n    rays_o, rays_d = get_rays(H, W, focal, testpose)\n    rgb, depth, acc = render_rays(model, rays_o, rays_d, near=2., far=6., N_samples=config_loader.N_samples, chunk_size=config_loader.chunk_size)\n\n    calculate_and_save_training_metrics(rgb, testimg, iteration_number)\n\n    loss = calculate_loss(rgb,testimg)\n    if has_to_plot(iteration_number, config_loader.iterations_to_plot_image):\n        print(iteration_number, (time.time() - start_time) / config_loader.iterations_to_plot_image, 'secs per iter ')\n        print('Loss ', loss.item())\n        generate_checkpoint(model, iteration_number, loss.item())\n        start_time = time.time()\n        # render_image(H, W, focal, testpose, testimg, model, i, in_training = True)\n        plot_image_and_metrics(rgb, iteration_number)","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"If you have any checkpoint for this specific model and you wish to continue training specify the path to obtain the .pt file to continue training (leave empty to use default checkpoint on output folder of the notebook)","metadata":{"editable":false}},{"cell_type":"code","source":"checkpoint_path = input('Path to checkpoint.pt')\n\nif (checkpoint_path == ''):\n    checkpoint_path = '/kaggle/working/'+exp_name+'_checkpoint.pt'\n    print(\"Default path \",'/kaggle/working/'+exp_name+'_checkpoint.pt')\nelse:\n    print(\"Checkpoint path setted to \",checkpoint_path)","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Train model","metadata":{"editable":false}},{"cell_type":"markdown","source":"","metadata":{"editable":false}},{"cell_type":"code","source":"# Create model\nmodel = TinyNerfModel(config_loader.L_embed)\nmodel.to(device)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=config_loader.learning_rate)\n\n# Load checkpoint if it exists\n\niteration_number_checkpoint = load_checkpoint(model,checkpoint_path)\n\n# EStas dos ahora estÃ¡n en el monitor.\n# psnrs = []\n# iternums = []\n\ntraining_monitor = TrainingMonitor(exp_name) # cambiamos el nombre de la clase y de la variable. \n\n# seed = 9458\n# torch.manual_seed(seed)\n# np.random.seed(seed)\ntraining_monitor.save_exp_configuration(config_loader);\nstart_time = time.time()\n\n\nfor i in range(config_loader.N_iters+1): # acÃ¡ borrÃ© un +1 en el parÃ¡metro\n\n    training_stage(H, W, focal, model)\n\n    testing_stage(H, W, focal, model, testimg, testpose, iteration_number_checkpoint + i, start_time)\n\n    # img_i = np.random.randint(images.shape[0])\n\n    # # esto lo pusimos asÃ­ por las dudas, para el futuro.\n    # if config_loader.image_selection_mode == 'random':\n    #     image_index = get_image_random_index(images)\n    # elif config_loader.image_selection_mode == 'angle':\n    #     image_index =  get_image_random_index(images) # <<placeholder, estÃ¡ igual que el then.\n\n    # target_image = get_image_by_index(image_index)\n    # target_image_pose = get_pose_by_index(image_index)\n    # target_image.to(device)\n    # target_image_pose.to(device)\n    # rays_o, rays_d = get_rays(H, W, focal, target_image_pose)\n    \n    # target = images[img_i].to(device)\n    # pose = poses[img_i].to(device)\n    # rays_o, rays_d = get_rays(H, W, focal, pose)\n    \n    # rgb, depth, acc = render_rays(model, rays_o, rays_d, near=2., far=6., N_samples=config_loader.N_samples, chunk_size=config_loader.chunk_size, rand=True)\n    # loss = torch.nn.functional.mse_loss(rgb, target_image)\n    # loss.backward()\n    # optimizer.step()\n    # optimizer.zero_grad()\n\n    # # testing with holdout image\n    # rays_o, rays_d = get_rays(H, W, focal, testpose)\n    # rgb, depth, acc = render_rays(model, rays_o, rays_d, near=2., far=6., N_samples=config_loader.N_samples, chunk_size=config_loader.chunk_size)\n\n    # calculate_and_save_training_metrics(rgb, testimg, i)\n\n    # # cambiamos esto para tener una Ãºnica funciÃ³n render_image\n    # if has_to_plot(i, config_loader.iterations_to_plot_image):\n    #     print(i, (time.time() - start_time) / config_loader.iterations_to_plot_image, 'secs per iter ')\n    #     print('Loss ',loss.item())\n    #     start_time = time.time()\n    #     render_image(H, W, focal, testpose, testimg, model, i, in_training = True)\n\n    \n\n\n    # Peak signal-to-noise-ratio metric for comparison\n    # psnr = -10. * torch.log10(loss)\n\n    # store information with the monitor.\n    # training_monitor.store_snapshot(i, loss.item(), psnr.item())\n    \n    # if has_to_plot(i, config_loader.iterations_to_plot_image):\n        # Render the holdout view for logging\n        # render_image_training(H,W, focal, testpose, testimg, model)\n        \nprint('Done')","metadata":{"id":"6XurcHoCj0FQ","outputId":"fac34e31-85ff-4226-b4e0-cab616245459","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Interactive Visualization","metadata":{"id":"bZLEFNox_UVK","editable":false}},{"cell_type":"code","source":"%matplotlib inline\nfrom ipywidgets import interactive, widgets\n\n\ntrans_t = lambda t : torch.tensor([\n    [1,0,0,0],\n    [0,1,0,0],\n    [0,0,1,t],\n    [0,0,0,1],\n], dtype=torch.float32, device=device)\n\nrot_phi = lambda phi : torch.as_tensor([\n    [1,0,0,0],\n    [0,torch.cos(phi),-torch.sin(phi),0],\n    [0,torch.sin(phi), torch.cos(phi),0],\n    [0,0,0,1],\n], dtype=torch.float32, device=device)\n\nrot_theta = lambda th : torch.as_tensor([\n    [torch.cos(th),0,-torch.sin(th),0],\n    [0,1,0,0],\n    [torch.sin(th),0, torch.cos(th),0],\n    [0,0,0,1],\n], dtype=torch.float32, device=device)\n\n\ndef pose_spherical(theta, phi, radius):\n    phi = torch.tensor(phi)\n    theta = torch.tensor(theta)\n    c2w = trans_t(radius)\n    c2w = rot_phi(phi/180.*np.pi) @ c2w\n    c2w = rot_theta(theta/180.*np.pi) @ c2w\n    c2w = torch.tensor([[-1, 0, 0, 0],\n                        [0, 0, 1, 0],\n                        [0, 1, 0, 0],\n                        [0, 0, 0, 1]], dtype=torch.float32, device=device) @ c2w\n    return c2w\n\n\ndef f(theta, phi, radius):\n    c2w = pose_spherical(theta, phi, radius)\n    rays_o, rays_d = get_rays(H, W, focal, c2w[:3,:4])\n    rgb, depth, acc = render_rays(model, rays_o, rays_d, near=2., far=6., N_samples=config_loader.N_samples, chunk_size=config_loader.chunk_size)\n    \n    plt.figure(2, figsize=(20,6))\n    plt.imshow(rgb.detach().cpu())\n    plt.show()\n    \n\nsldr = lambda v, mi, ma: widgets.FloatSlider(\n    value=v,\n    min=mi,\n    max=ma,\n    step=.01,\n)\n\nnames = [\n    ['theta', [100., 0., 360]],\n    ['phi', [-30., -90, 0]],\n    ['radius', [4., 3., 5.]],\n]\n\ninteractive_plot = interactive(f, **{s[0] : sldr(*s[1]) for s in names})\noutput = interactive_plot.children[-1]\noutput.layout.height = '350px'\ninteractive_plot","metadata":{"id":"L92jHDI7j0FT","outputId":"758e5559-8b25-47ff-bfb6-4a6c088e01af","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Comparing model prediction vs Ground truth","metadata":{"editable":false}},{"cell_type":"code","source":"from IPython import display\n\ndef update_image(rgb_map, pose_index):\n    ax1.imshow(rgb_map.cpu().detach())\n    ax2.imshow(get_image_by_index(pose_index).cpu().detach())\n    plt.pause(time_between_images)\n    display.display(fig)\n    display.clear_output(wait=True)\n\ndef plot_image_by_pose_index(pose_index):\n    c2w = get_pose_by_index(pose_index)\n    rays_o, rays_d = get_rays(H, W, focal, c2w[:3,:4])\n    rgb, depth, acc = render_rays(model, rays_o, rays_d, near=2., far=6., N_samples=config_loader.N_samples, chunk_size=config_loader.chunk_size)  \n    update_image(rgb, pose_index)\n\ntime_between_images = 0.01\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 6))\nax1.set_title('Predicted')\nax2.set_title('Ground truth')\nfor i in range(1,len(poses)):\n    plot_image_by_pose_index(i)","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Render 360 Video","metadata":{"id":"PpKhAn2a__Iu","editable":false}},{"cell_type":"code","source":"frames = []\nfor th in tqdm(np.linspace(0., 360., 120, endpoint=False)):\n    c2w = pose_spherical(th, -30., 4.)\n    rays_o, rays_d = get_rays(H, W, focal, c2w[:3,:4])\n    rgb, depth, acc = render_rays(model, rays_o, rays_d, near=2., far=6., N_samples=config_loader.N_samples, chunk_size=config_loader.chunk_size)\n    rgb = rgb.detach().cpu().numpy()\n    parsed_img = (255*np.clip(rgb,0,1)).astype(np.uint8)\n    \n    # We need to parse the image at the end for the color to appear correctly\n    frame = cv2.cvtColor(parsed_img, cv2.COLOR_RGB2BGR)\n    \n    frames.append(frame)\n\nimport imageio as iio\nf = 'video.mp4'\n\n# Video encoding\nfourcc = cv2.VideoWriter_fourcc(*'vp09') # We can also use mp4v but it doesn't show on the cell below\nvideo = cv2.VideoWriter(f, fourcc, 30, (W,H))\nfor frame in frames:\n    video.write(frame)\nvideo.release()\n","metadata":{"id":"8Sg4aV0cmVPs","outputId":"b88362ee-8e98-4ac1-c704-59dc9df5b283","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Display Video","metadata":{"editable":false}},{"cell_type":"code","source":"from IPython.display import HTML\nfrom base64 import b64encode\nmp4 = open('video.mp4','rb').read()\ndata_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\nHTML(\"\"\"\n<video width=400 controls autoplay loop>\n      <source src=\"%s\" type=\"video/mp4\">\n</video>\n\"\"\" % data_url)","metadata":{"id":"OQ_ms-YMyFly","outputId":"5354e038-5cc1-4ac5-88e7-8a34849cc7e4","trusted":true,"editable":false},"outputs":[],"execution_count":null}]}